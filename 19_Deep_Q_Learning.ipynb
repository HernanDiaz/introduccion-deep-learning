{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPMlxD2qhwYxKEcow/hldLx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XRQI818ij5JX","executionInfo":{"status":"ok","timestamp":1761483946292,"user_tz":-60,"elapsed":435711,"user":{"displayName":"Hernan Diaz Rodriguez","userId":"01995553725561234934"}},"outputId":"cac2c9d5-1703-45d6-ce27-ed873507c332"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 1: total_reward=39.0, epsilon=0.995\n","Episode 2: total_reward=11.0, epsilon=0.990\n","Episode 3: total_reward=45.0, epsilon=0.985\n","Episode 4: total_reward=15.0, epsilon=0.980\n","Episode 5: total_reward=13.0, epsilon=0.975\n","Episode 6: total_reward=30.0, epsilon=0.970\n","Episode 7: total_reward=28.0, epsilon=0.966\n","Episode 8: total_reward=21.0, epsilon=0.961\n","Episode 9: total_reward=16.0, epsilon=0.956\n","Episode 10: total_reward=20.0, epsilon=0.951\n","Episode 11: total_reward=23.0, epsilon=0.946\n","Episode 12: total_reward=20.0, epsilon=0.942\n","Episode 13: total_reward=28.0, epsilon=0.937\n","Episode 14: total_reward=41.0, epsilon=0.932\n","Episode 15: total_reward=32.0, epsilon=0.928\n","Episode 16: total_reward=34.0, epsilon=0.923\n","Episode 17: total_reward=10.0, epsilon=0.918\n","Episode 18: total_reward=17.0, epsilon=0.914\n","Episode 19: total_reward=12.0, epsilon=0.909\n","Episode 20: total_reward=12.0, epsilon=0.905\n","Episode 21: total_reward=47.0, epsilon=0.900\n","Episode 22: total_reward=27.0, epsilon=0.896\n","Episode 23: total_reward=23.0, epsilon=0.891\n","Episode 24: total_reward=15.0, epsilon=0.887\n","Episode 25: total_reward=22.0, epsilon=0.882\n","Episode 26: total_reward=23.0, epsilon=0.878\n","Episode 27: total_reward=24.0, epsilon=0.873\n","Episode 28: total_reward=14.0, epsilon=0.869\n","Episode 29: total_reward=11.0, epsilon=0.865\n","Episode 30: total_reward=16.0, epsilon=0.860\n","Episode 31: total_reward=33.0, epsilon=0.856\n","Episode 32: total_reward=19.0, epsilon=0.852\n","Episode 33: total_reward=12.0, epsilon=0.848\n","Episode 34: total_reward=26.0, epsilon=0.843\n","Episode 35: total_reward=17.0, epsilon=0.839\n","Episode 36: total_reward=14.0, epsilon=0.835\n","Episode 37: total_reward=10.0, epsilon=0.831\n","Episode 38: total_reward=82.0, epsilon=0.827\n","Episode 39: total_reward=61.0, epsilon=0.822\n","Episode 40: total_reward=81.0, epsilon=0.818\n","Episode 41: total_reward=42.0, epsilon=0.814\n","Episode 42: total_reward=12.0, epsilon=0.810\n","Episode 43: total_reward=22.0, epsilon=0.806\n","Episode 44: total_reward=59.0, epsilon=0.802\n","Episode 45: total_reward=34.0, epsilon=0.798\n","Episode 46: total_reward=34.0, epsilon=0.794\n","Episode 47: total_reward=22.0, epsilon=0.790\n","Episode 48: total_reward=19.0, epsilon=0.786\n","Episode 49: total_reward=47.0, epsilon=0.782\n","Episode 50: total_reward=38.0, epsilon=0.778\n","Episode 51: total_reward=23.0, epsilon=0.774\n","Episode 52: total_reward=21.0, epsilon=0.771\n","Episode 53: total_reward=99.0, epsilon=0.767\n","Episode 54: total_reward=39.0, epsilon=0.763\n","Episode 55: total_reward=76.0, epsilon=0.759\n","Episode 56: total_reward=26.0, epsilon=0.755\n","Episode 57: total_reward=83.0, epsilon=0.751\n","Episode 58: total_reward=26.0, epsilon=0.748\n","Episode 59: total_reward=135.0, epsilon=0.744\n","Episode 60: total_reward=33.0, epsilon=0.740\n","Episode 61: total_reward=17.0, epsilon=0.737\n","Episode 62: total_reward=14.0, epsilon=0.733\n","Episode 63: total_reward=77.0, epsilon=0.729\n","Episode 64: total_reward=38.0, epsilon=0.726\n","Episode 65: total_reward=67.0, epsilon=0.722\n","Episode 66: total_reward=22.0, epsilon=0.718\n","Episode 67: total_reward=54.0, epsilon=0.715\n","Episode 68: total_reward=21.0, epsilon=0.711\n","Episode 69: total_reward=161.0, epsilon=0.708\n","Episode 70: total_reward=72.0, epsilon=0.704\n","Episode 71: total_reward=49.0, epsilon=0.701\n","Episode 72: total_reward=48.0, epsilon=0.697\n","Episode 73: total_reward=8.0, epsilon=0.694\n","Episode 74: total_reward=118.0, epsilon=0.690\n","Episode 75: total_reward=43.0, epsilon=0.687\n","Episode 76: total_reward=25.0, epsilon=0.683\n","Episode 77: total_reward=98.0, epsilon=0.680\n","Episode 78: total_reward=72.0, epsilon=0.676\n","Episode 79: total_reward=52.0, epsilon=0.673\n","Episode 80: total_reward=20.0, epsilon=0.670\n","Episode 81: total_reward=20.0, epsilon=0.666\n","Episode 82: total_reward=129.0, epsilon=0.663\n","Episode 83: total_reward=25.0, epsilon=0.660\n","Episode 84: total_reward=11.0, epsilon=0.656\n","Episode 85: total_reward=47.0, epsilon=0.653\n","Episode 86: total_reward=69.0, epsilon=0.650\n","Episode 87: total_reward=70.0, epsilon=0.647\n","Episode 88: total_reward=36.0, epsilon=0.643\n","Episode 89: total_reward=37.0, epsilon=0.640\n","Episode 90: total_reward=99.0, epsilon=0.637\n","Episode 91: total_reward=41.0, epsilon=0.634\n","Episode 92: total_reward=21.0, epsilon=0.631\n","Episode 93: total_reward=117.0, epsilon=0.627\n","Episode 94: total_reward=85.0, epsilon=0.624\n","Episode 95: total_reward=45.0, epsilon=0.621\n","Episode 96: total_reward=101.0, epsilon=0.618\n","Episode 97: total_reward=80.0, epsilon=0.615\n","Episode 98: total_reward=26.0, epsilon=0.612\n","Episode 99: total_reward=95.0, epsilon=0.609\n","Episode 100: total_reward=30.0, epsilon=0.606\n","Episode 101: total_reward=27.0, epsilon=0.603\n","Episode 102: total_reward=19.0, epsilon=0.600\n","Episode 103: total_reward=14.0, epsilon=0.597\n","Episode 104: total_reward=49.0, epsilon=0.594\n","Episode 105: total_reward=139.0, epsilon=0.591\n","Episode 106: total_reward=71.0, epsilon=0.588\n","Episode 107: total_reward=95.0, epsilon=0.585\n","Episode 108: total_reward=151.0, epsilon=0.582\n","Episode 109: total_reward=132.0, epsilon=0.579\n","Episode 110: total_reward=21.0, epsilon=0.576\n","Episode 111: total_reward=161.0, epsilon=0.573\n","Episode 112: total_reward=172.0, epsilon=0.570\n","Episode 113: total_reward=225.0, epsilon=0.568\n","Episode 114: total_reward=41.0, epsilon=0.565\n","Episode 115: total_reward=15.0, epsilon=0.562\n","Episode 116: total_reward=108.0, epsilon=0.559\n","Episode 117: total_reward=19.0, epsilon=0.556\n","Episode 118: total_reward=188.0, epsilon=0.554\n","Episode 119: total_reward=88.0, epsilon=0.551\n","Episode 120: total_reward=49.0, epsilon=0.548\n","Episode 121: total_reward=75.0, epsilon=0.545\n","Episode 122: total_reward=135.0, epsilon=0.543\n","Episode 123: total_reward=166.0, epsilon=0.540\n","Episode 124: total_reward=125.0, epsilon=0.537\n","Episode 125: total_reward=183.0, epsilon=0.534\n","Episode 126: total_reward=282.0, epsilon=0.532\n","Episode 127: total_reward=34.0, epsilon=0.529\n","Episode 128: total_reward=53.0, epsilon=0.526\n","Episode 129: total_reward=24.0, epsilon=0.524\n","Episode 130: total_reward=115.0, epsilon=0.521\n","Episode 131: total_reward=19.0, epsilon=0.519\n","Episode 132: total_reward=129.0, epsilon=0.516\n","Episode 133: total_reward=61.0, epsilon=0.513\n","Episode 134: total_reward=47.0, epsilon=0.511\n","Episode 135: total_reward=256.0, epsilon=0.508\n","Episode 136: total_reward=70.0, epsilon=0.506\n","Episode 137: total_reward=128.0, epsilon=0.503\n","Episode 138: total_reward=53.0, epsilon=0.501\n","Episode 139: total_reward=388.0, epsilon=0.498\n","Episode 140: total_reward=208.0, epsilon=0.496\n","Episode 141: total_reward=127.0, epsilon=0.493\n","Episode 142: total_reward=294.0, epsilon=0.491\n","Episode 143: total_reward=198.0, epsilon=0.488\n","Episode 144: total_reward=206.0, epsilon=0.486\n","Episode 145: total_reward=20.0, epsilon=0.483\n","Episode 146: total_reward=184.0, epsilon=0.481\n","Episode 147: total_reward=119.0, epsilon=0.479\n","Episode 148: total_reward=66.0, epsilon=0.476\n","Episode 149: total_reward=62.0, epsilon=0.474\n","Episode 150: total_reward=304.0, epsilon=0.471\n","Episode 151: total_reward=109.0, epsilon=0.469\n","Episode 152: total_reward=191.0, epsilon=0.467\n","Episode 153: total_reward=190.0, epsilon=0.464\n","Episode 154: total_reward=102.0, epsilon=0.462\n","Episode 155: total_reward=138.0, epsilon=0.460\n","Episode 156: total_reward=219.0, epsilon=0.458\n","Episode 157: total_reward=82.0, epsilon=0.455\n","Episode 158: total_reward=283.0, epsilon=0.453\n","Episode 159: total_reward=52.0, epsilon=0.451\n","Episode 160: total_reward=157.0, epsilon=0.448\n","Episode 161: total_reward=236.0, epsilon=0.446\n","Episode 162: total_reward=99.0, epsilon=0.444\n","Episode 163: total_reward=294.0, epsilon=0.442\n","Episode 164: total_reward=235.0, epsilon=0.440\n","Episode 165: total_reward=363.0, epsilon=0.437\n","Episode 166: total_reward=158.0, epsilon=0.435\n","Episode 167: total_reward=202.0, epsilon=0.433\n","Episode 168: total_reward=123.0, epsilon=0.431\n","Episode 169: total_reward=39.0, epsilon=0.429\n","Episode 170: total_reward=287.0, epsilon=0.427\n","Episode 171: total_reward=173.0, epsilon=0.424\n","Episode 172: total_reward=265.0, epsilon=0.422\n","Episode 173: total_reward=51.0, epsilon=0.420\n","Episode 174: total_reward=323.0, epsilon=0.418\n","Episode 175: total_reward=220.0, epsilon=0.416\n","Episode 176: total_reward=22.0, epsilon=0.414\n","Episode 177: total_reward=42.0, epsilon=0.412\n","Episode 178: total_reward=307.0, epsilon=0.410\n","Episode 179: total_reward=161.0, epsilon=0.408\n","Episode 180: total_reward=284.0, epsilon=0.406\n","Episode 181: total_reward=190.0, epsilon=0.404\n","Episode 182: total_reward=43.0, epsilon=0.402\n","Episode 183: total_reward=292.0, epsilon=0.400\n","Episode 184: total_reward=276.0, epsilon=0.398\n","Episode 185: total_reward=84.0, epsilon=0.396\n","Episode 186: total_reward=169.0, epsilon=0.394\n","Episode 187: total_reward=26.0, epsilon=0.392\n","Episode 188: total_reward=312.0, epsilon=0.390\n","Episode 189: total_reward=211.0, epsilon=0.388\n","Episode 190: total_reward=399.0, epsilon=0.386\n","Episode 191: total_reward=243.0, epsilon=0.384\n","Episode 192: total_reward=339.0, epsilon=0.382\n","Episode 193: total_reward=349.0, epsilon=0.380\n","Episode 194: total_reward=375.0, epsilon=0.378\n","Episode 195: total_reward=119.0, epsilon=0.376\n","Episode 196: total_reward=141.0, epsilon=0.374\n","Episode 197: total_reward=270.0, epsilon=0.373\n","Episode 198: total_reward=377.0, epsilon=0.371\n","Episode 199: total_reward=168.0, epsilon=0.369\n","Episode 200: total_reward=72.0, epsilon=0.367\n","[TEST] Episodio 1: recompensa total = 294.0\n","[TEST] Episodio 2: recompensa total = 296.0\n","[TEST] Episodio 3: recompensa total = 272.0\n","[TEST] Episodio 4: recompensa total = 284.0\n","[TEST] Episodio 5: recompensa total = 319.0\n"]}],"source":["import random\n","import collections\n","import numpy as np\n","import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# ---------------------------\n","# Red neuronal para Q(s,a)\n","# ---------------------------\n","class DQN(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super(DQN, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(state_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, action_dim)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","\n","# ---------------------------\n","# Replay Buffer\n","# ---------------------------\n","class ReplayBuffer:\n","    def __init__(self, capacity=10000):\n","        self.buffer = collections.deque(maxlen=capacity)\n","\n","    def push(self, state, action, reward, next_state, done):\n","        self.buffer.append((state, action, reward, next_state, done))\n","\n","    def sample(self, batch_size):\n","        batch = random.sample(self.buffer, batch_size)\n","        states, actions, rewards, next_states, dones = zip(*batch)\n","        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\n","                np.array(next_states), np.array(dones, dtype=np.float32))\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","\n","# ---------------------------\n","# Entrenamiento DQN\n","# ---------------------------\n","def train_dqn(env, episodes=500, gamma=0.99, batch_size=64, lr=1e-3):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    state_dim = env.observation_space.shape[0]\n","    action_dim = env.action_space.n\n","\n","    q_net = DQN(state_dim, action_dim).to(device)\n","    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n","    replay_buffer = ReplayBuffer()\n","\n","    epsilon = 1.0\n","    epsilon_min = 0.01\n","    epsilon_decay = 0.995\n","\n","    for ep in range(episodes):\n","        state, _ = env.reset()\n","        total_reward = 0\n","\n","        while True:\n","            # Política epsilon-greedy\n","            if random.random() < epsilon:\n","                action = env.action_space.sample()\n","            else:\n","                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n","                q_values = q_net(state_tensor)\n","                action = q_values.argmax().item()\n","\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","\n","            replay_buffer.push(state, action, reward, next_state, done)\n","            state = next_state\n","            total_reward += reward\n","\n","            # Entrenamiento\n","            if len(replay_buffer) >= batch_size:\n","                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n","\n","                states = torch.FloatTensor(states).to(device)\n","                actions = torch.LongTensor(actions).to(device)\n","                rewards = torch.FloatTensor(rewards).to(device)\n","                next_states = torch.FloatTensor(next_states).to(device)\n","                dones = torch.FloatTensor(dones).to(device)\n","\n","                q_values = q_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n","                next_q_values = q_net(next_states).max(1)[0]\n","                target = rewards + gamma * next_q_values * (1 - dones)\n","\n","                loss = nn.MSELoss()(q_values, target.detach())\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","            if done:\n","                break\n","\n","        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n","        print(f\"Episode {ep+1}: total_reward={total_reward}, epsilon={epsilon:.3f}\")\n","\n","    return q_net\n","\n","# ---------------------------\n","# Evaluación del agente entrenado\n","# ---------------------------\n","def test_dqn(env, q_net, episodes=5):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    q_net.eval()  # modo evaluación\n","    for ep in range(episodes):\n","        state, _ = env.reset()\n","        total_reward = 0\n","        done = False\n","        while not done:\n","            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n","            with torch.no_grad():\n","                q_values = q_net(state_tensor)\n","                action = q_values.argmax().item()\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","            done = terminated or truncated\n","            state = next_state\n","            total_reward += reward\n","        print(f\"[TEST] Episodio {ep+1}: recompensa total = {total_reward}\")\n","\n","\n","\n","# ---------------------------\n","# Main\n","# ---------------------------\n","if __name__ == \"__main__\":\n","    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n","    trained_qnet = train_dqn(env, episodes=200)\n","\n","    # Probar la red entrenada\n","    test_dqn(env, trained_qnet, episodes=5)"]}]}
