{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNIjOhWIJQqunsfgF+iZw/w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6TQuq7QfFu7p","executionInfo":{"status":"ok","timestamp":1761158424988,"user_tz":-120,"elapsed":2196954,"user":{"displayName":"Hernan Diaz Rodriguez","userId":"01995553725561234934"}},"outputId":"e6237327-412c-4411-ff0c-efaae38363b4"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 221MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 32.7MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 31.8MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 6.81MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training on [cpu]...\n","Epoch [1/100] - D_loss: 1.1602, G_loss: 1.8300\n","Epoch [2/100] - D_loss: 1.2673, G_loss: 1.0781\n","Epoch [3/100] - D_loss: 1.1982, G_loss: 1.0727\n","Epoch [4/100] - D_loss: 1.1126, G_loss: 1.3674\n","Epoch [5/100] - D_loss: 1.1161, G_loss: 1.2542\n","Epoch [6/100] - D_loss: 1.1521, G_loss: 1.2489\n","Epoch [7/100] - D_loss: 1.1074, G_loss: 1.3390\n","Epoch [8/100] - D_loss: 1.1356, G_loss: 1.1910\n","Epoch [9/100] - D_loss: 1.1379, G_loss: 1.1535\n","Epoch [10/100] - D_loss: 1.1910, G_loss: 1.0719\n","Epoch [11/100] - D_loss: 1.2027, G_loss: 1.0357\n","Epoch [12/100] - D_loss: 1.2057, G_loss: 1.0187\n","Epoch [13/100] - D_loss: 1.2086, G_loss: 1.0276\n","Epoch [14/100] - D_loss: 1.2144, G_loss: 0.9990\n","Epoch [15/100] - D_loss: 1.2322, G_loss: 0.9690\n","Epoch [16/100] - D_loss: 1.2292, G_loss: 0.9811\n","Epoch [17/100] - D_loss: 1.2278, G_loss: 0.9831\n","Epoch [18/100] - D_loss: 1.2296, G_loss: 0.9782\n","Epoch [19/100] - D_loss: 1.2291, G_loss: 0.9886\n","Epoch [20/100] - D_loss: 1.2430, G_loss: 0.9702\n","Epoch [21/100] - D_loss: 1.2430, G_loss: 0.9749\n","Epoch [22/100] - D_loss: 1.2528, G_loss: 0.9402\n","Epoch [23/100] - D_loss: 1.2475, G_loss: 0.9640\n","Epoch [24/100] - D_loss: 1.2354, G_loss: 0.9501\n","Epoch [25/100] - D_loss: 1.2544, G_loss: 0.9425\n","Epoch [26/100] - D_loss: 1.2597, G_loss: 0.9375\n","Epoch [27/100] - D_loss: 1.2470, G_loss: 0.9737\n","Epoch [28/100] - D_loss: 1.2371, G_loss: 0.9737\n","Epoch [29/100] - D_loss: 1.2539, G_loss: 0.9480\n","Epoch [30/100] - D_loss: 1.2392, G_loss: 0.9814\n","Epoch [31/100] - D_loss: 1.2428, G_loss: 0.9683\n","Epoch [32/100] - D_loss: 1.2488, G_loss: 0.9480\n","Epoch [33/100] - D_loss: 1.2573, G_loss: 0.9357\n","Epoch [34/100] - D_loss: 1.2572, G_loss: 0.9467\n","Epoch [35/100] - D_loss: 1.2450, G_loss: 0.9640\n","Epoch [36/100] - D_loss: 1.2614, G_loss: 0.9370\n","Epoch [37/100] - D_loss: 1.2468, G_loss: 0.9727\n","Epoch [38/100] - D_loss: 1.2461, G_loss: 0.9559\n","Epoch [39/100] - D_loss: 1.2567, G_loss: 0.9537\n","Epoch [40/100] - D_loss: 1.2464, G_loss: 0.9635\n","Epoch [41/100] - D_loss: 1.2562, G_loss: 0.9304\n","Epoch [42/100] - D_loss: 1.2761, G_loss: 0.9165\n","Epoch [43/100] - D_loss: 1.2373, G_loss: 1.0043\n","Epoch [44/100] - D_loss: 1.2419, G_loss: 0.9487\n","Epoch [45/100] - D_loss: 1.2472, G_loss: 0.9616\n","Epoch [46/100] - D_loss: 1.2480, G_loss: 0.9655\n","Epoch [47/100] - D_loss: 1.2532, G_loss: 0.9372\n","Epoch [48/100] - D_loss: 1.2379, G_loss: 0.9786\n","Epoch [49/100] - D_loss: 1.2367, G_loss: 0.9763\n","Epoch [50/100] - D_loss: 1.2574, G_loss: 0.9214\n","Epoch [51/100] - D_loss: 1.2500, G_loss: 0.9582\n","Epoch [52/100] - D_loss: 1.2375, G_loss: 0.9869\n","Epoch [53/100] - D_loss: 1.2322, G_loss: 0.9584\n","Epoch [54/100] - D_loss: 1.2404, G_loss: 0.9486\n","Epoch [55/100] - D_loss: 1.2381, G_loss: 0.9868\n","Epoch [56/100] - D_loss: 1.2379, G_loss: 0.9598\n","Epoch [57/100] - D_loss: 1.2350, G_loss: 0.9775\n","Epoch [58/100] - D_loss: 1.2240, G_loss: 0.9739\n","Epoch [59/100] - D_loss: 1.2410, G_loss: 0.9557\n","Epoch [60/100] - D_loss: 1.2190, G_loss: 1.0251\n","Epoch [61/100] - D_loss: 1.2237, G_loss: 0.9801\n","Epoch [62/100] - D_loss: 1.2303, G_loss: 0.9548\n","Epoch [63/100] - D_loss: 1.2368, G_loss: 0.9739\n","Epoch [64/100] - D_loss: 1.2460, G_loss: 0.9499\n","Epoch [65/100] - D_loss: 1.2273, G_loss: 0.9778\n","Epoch [66/100] - D_loss: 1.2336, G_loss: 0.9888\n","Epoch [67/100] - D_loss: 1.2310, G_loss: 0.9861\n","Epoch [68/100] - D_loss: 1.2324, G_loss: 0.9765\n","Epoch [69/100] - D_loss: 1.2256, G_loss: 1.0107\n","Epoch [70/100] - D_loss: 1.2075, G_loss: 1.0238\n","Epoch [71/100] - D_loss: 1.2217, G_loss: 0.9928\n","Epoch [72/100] - D_loss: 1.2207, G_loss: 0.9975\n","Epoch [73/100] - D_loss: 1.2206, G_loss: 0.9965\n","Epoch [74/100] - D_loss: 1.2143, G_loss: 1.0121\n","Epoch [75/100] - D_loss: 1.2176, G_loss: 1.0194\n","Epoch [76/100] - D_loss: 1.2162, G_loss: 1.0067\n","Epoch [77/100] - D_loss: 1.2211, G_loss: 0.9946\n","Epoch [78/100] - D_loss: 1.2174, G_loss: 1.0132\n","Epoch [79/100] - D_loss: 1.2055, G_loss: 1.0323\n","Epoch [80/100] - D_loss: 1.2060, G_loss: 1.0268\n","Epoch [81/100] - D_loss: 1.2065, G_loss: 1.0268\n","Epoch [82/100] - D_loss: 1.2084, G_loss: 1.0386\n","Epoch [83/100] - D_loss: 1.1980, G_loss: 1.0326\n","Epoch [84/100] - D_loss: 1.2124, G_loss: 1.0140\n","Epoch [85/100] - D_loss: 1.2051, G_loss: 1.0339\n","Epoch [86/100] - D_loss: 1.2025, G_loss: 1.0395\n","Epoch [87/100] - D_loss: 1.2032, G_loss: 1.0565\n","Epoch [88/100] - D_loss: 1.2081, G_loss: 1.0323\n","Epoch [89/100] - D_loss: 1.2065, G_loss: 1.0337\n","Epoch [90/100] - D_loss: 1.2031, G_loss: 1.0281\n","Epoch [91/100] - D_loss: 1.2101, G_loss: 1.0320\n","Epoch [92/100] - D_loss: 1.2018, G_loss: 1.0422\n","Epoch [93/100] - D_loss: 1.1985, G_loss: 1.0375\n","Epoch [94/100] - D_loss: 1.1998, G_loss: 1.0521\n","Epoch [95/100] - D_loss: 1.1981, G_loss: 1.0317\n","Epoch [96/100] - D_loss: 1.2041, G_loss: 1.0303\n","Epoch [97/100] - D_loss: 1.2048, G_loss: 1.0364\n","Epoch [98/100] - D_loss: 1.1827, G_loss: 1.0786\n","Epoch [99/100] - D_loss: 1.1917, G_loss: 1.0516\n","Epoch [100/100] - D_loss: 1.1920, G_loss: 1.0610\n"]}],"source":["import math\n","import pickle as pkl\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","# Cargar datos\n","transform = transforms.Compose([transforms.ToTensor()])\n","train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","dl = DataLoader(dataset=train_ds, shuffle=True, batch_size=64)\n","\n","# Arquitecturas\n","class Discriminator(nn.Module):\n","    def __init__(self, in_features=784, out_features=1):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(in_features, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, 64),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 32),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(32, out_features)\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.shape[0]\n","        x = x.view(batch_size, -1)\n","        return self.model(x)\n","\n","class Generator(nn.Module):\n","    def __init__(self, in_features=100, out_features=784):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(in_features, 32),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(32, 64),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(64, 128),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(0.3),\n","            nn.Linear(128, out_features),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Funciones de pérdida\n","def compute_loss(logits, target_value, loss_fn, device):\n","    \"\"\"Calcula pérdida para outputs reales (target=1) o falsos (target=0)\"\"\"\n","    batch_size = logits.shape[0]\n","    targets = torch.full((batch_size,), target_value, device=device)\n","    return loss_fn(logits.squeeze(), targets)\n","\n","# Entrenamiento\n","def train_gan(d, g, d_optim, g_optim, loss_fn, dl, n_epochs, device, z_size=100):\n","    print(f'Training on [{device}]...')\n","\n","    # Vector latente fijo para monitorear progreso\n","    fixed_z = torch.randn(16, z_size, device=device)\n","    fixed_samples = []\n","    d_losses, g_losses = [], []\n","\n","    d.to(device)\n","    g.to(device)\n","\n","    for epoch in range(n_epochs):\n","        d.train()\n","        g.train()\n","        d_running_loss = 0\n","        g_running_loss = 0\n","\n","        for real_images, _ in dl:\n","            real_images = real_images.to(device)\n","            batch_size = real_images.size(0)\n","\n","            # Entrenar Discriminador\n","            d_optim.zero_grad()\n","\n","            # Con imágenes reales (escalar a [-1, 1])\n","            real_images = (real_images * 2) - 1\n","            d_real_out = d(real_images)\n","            d_real_loss = compute_loss(d_real_out, 1.0, loss_fn, device)\n","\n","            # Con imágenes falsas\n","            z = torch.randn(batch_size, z_size, device=device)\n","            fake_images = g(z).detach()\n","            d_fake_out = d(fake_images)\n","            d_fake_loss = compute_loss(d_fake_out, 0.0, loss_fn, device)\n","\n","            d_loss = d_real_loss + d_fake_loss\n","            d_loss.backward()\n","            d_optim.step()\n","            d_running_loss += d_loss.item()\n","\n","            # Entrenar Generador\n","            g_optim.zero_grad()\n","\n","            z = torch.randn(batch_size, z_size, device=device)\n","            fake_images = g(z)\n","            g_out = d(fake_images)\n","            g_loss = compute_loss(g_out, 1.0, loss_fn, device)\n","\n","            g_loss.backward()\n","            g_optim.step()\n","            g_running_loss += g_loss.item()\n","\n","        # Guardar pérdidas por época\n","        d_losses.append(d_running_loss / len(dl))\n","        g_losses.append(g_running_loss / len(dl))\n","\n","        print(f'Epoch [{epoch+1}/{n_epochs}] - D_loss: {d_losses[-1]:.4f}, G_loss: {g_losses[-1]:.4f}')\n","\n","        # Guardar muestras del generador\n","        g.eval()\n","        with torch.no_grad():\n","            fixed_samples.append(g(fixed_z).cpu())\n","\n","    # Guardar muestras fijas\n","    with open('fixed_samples.pkl', 'wb') as f:\n","        pkl.dump(fixed_samples, f)\n","\n","    return d_losses, g_losses\n","\n","# Configuración y entrenamiento\n","d = Discriminator()\n","g = Generator()\n","\n","d_optim = optim.Adam(d.parameters(), lr=0.002)\n","g_optim = optim.Adam(g.parameters(), lr=0.002)\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Entrenar\n","n_epochs = 100\n","d_losses, g_losses = train_gan(d, g, d_optim, g_optim, loss_fn, dl, n_epochs, device)"]}]}