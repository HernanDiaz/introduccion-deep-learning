{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvq3S/kHte/LSfdubT4+9a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k27o9s6u9jQu","executionInfo":{"status":"ok","timestamp":1761039118709,"user_tz":-120,"elapsed":60245,"user":{"displayName":"Hernan Diaz Rodriguez","userId":"01995553725561234934"}},"outputId":"159869bc-fd01-4fa3-cb3f-5886f3a4fbd2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n","Dataset size: 134\n","Vocab size: 26\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 | Train loss 0.277\n","Epoch 20 | Train loss 0.131\n","Epoch 30 | Train loss 0.075\n","Epoch 40 | Train loss 0.068\n","Epoch 50 | Train loss 0.065\n","Epoch 60 | Train loss 0.059\n","Epoch 70 | Train loss 0.075\n","Epoch 80 | Train loss 0.055\n","Epoch 90 | Train loss 0.056\n","Epoch 100 | Train loss 0.044\n","cero            -> zero\n","quince          -> fiften\n","veintidos       -> twenty two\n","treinta y cinco -> thirty eighthth\n","noventa y nueve -> ninety nine\n"]}],"source":["# Mini-Transformer: traducir números (es -> en) a nivel de caracteres\n","# ----------------------------------------------------------\n","# Ejemplo didáctico: dataset pequeño (0–99), modelo Transformer sencillo\n","# Requiere: torch\n","# ----------------------------------------------------------\n","\n","import math, random\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# ----------------------------------------------------------\n","# 1. Dataset: números en texto (español -> inglés)\n","# ----------------------------------------------------------\n","UNITS_EN = [\"zero\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\n","            \"ten\",\"eleven\",\"twelve\",\"thirteen\",\"fourteen\",\"fifteen\",\"sixteen\",\n","            \"seventeen\",\"eighteen\",\"nineteen\"]\n","TENS_EN = [\"\",\"\",\"twenty\",\"thirty\",\"forty\",\"fifty\",\"sixty\",\"seventy\",\"eighty\",\"ninety\"]\n","\n","UNITS_ES = [\"cero\",\"uno\",\"dos\",\"tres\",\"cuatro\",\"cinco\",\"seis\",\"siete\",\"ocho\",\"nueve\",\n","            \"diez\",\"once\",\"doce\",\"trece\",\"catorce\",\"quince\",\"dieciseis\",\"diecisiete\",\n","            \"dieciocho\",\"diecinueve\"]\n","TENS_ES = [\"\",\"\",\"veinte\",\"treinta\",\"cuarenta\",\"cincuenta\",\"sesenta\",\"setenta\",\"ochenta\",\"noventa\"]\n","\n","def num_to_en(n:int) -> str:\n","    if n < 20: return UNITS_EN[n]\n","    tens, unit = n//10, n%10\n","    return TENS_EN[tens] if unit==0 else TENS_EN[tens]+\" \"+UNITS_EN[unit]\n","\n","def num_to_es(n:int) -> str:\n","    if n < 20: return UNITS_ES[n]\n","    tens, unit = n//10, n%10\n","    if unit==0: return TENS_ES[tens]\n","    if tens==2: return \"veinti\"+UNITS_ES[unit]  # veintiuno, veintidos...\n","    return TENS_ES[tens]+\" \"+UNITS_ES[unit]\n","\n","pairs = [(num_to_es(n), num_to_en(n)) for n in range(100)]\n","\n","# pequeña data augmentation: variantes y duplicados\n","aug_pairs = []\n","for n in range(21,30):\n","    aug_pairs.append((\"veinti \"+UNITS_ES[n%10], num_to_en(n)))\n","pairs += aug_pairs\n","pairs += [(src,tgt) for (src,tgt) in pairs if random.random()<0.2]\n","\n","random.shuffle(pairs)\n","print(\"Dataset size:\", len(pairs))\n","\n","# ----------------------------------------------------------\n","# 2. Tokenización carácter a carácter\n","# ----------------------------------------------------------\n","ALL_TEXT = \" \".join([s for s,_ in pairs] + [t for _,t in pairs])\n","chars = sorted(set(ALL_TEXT))\n","\n","PAD,BOS,EOS,UNK = \"<pad>\",\"<bos>\",\"<eos>\",\"<unk>\"\n","itos = [PAD,BOS,EOS,UNK]+chars\n","stoi = {ch:i for i,ch in enumerate(itos)}\n","\n","PAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX = stoi[PAD], stoi[BOS], stoi[EOS], stoi[UNK]\n","vocab_size = len(itos)\n","print(\"Vocab size:\", vocab_size)\n","\n","def encode(s): return [stoi.get(ch,UNK_IDX) for ch in s.lower()]\n","def decode(idxs):\n","    out=[]\n","    for i in idxs:\n","        if i in (PAD_IDX,BOS_IDX): continue\n","        if i==EOS_IDX: break\n","        out.append(itos[i])\n","    return \"\".join(out)\n","\n","# ----------------------------------------------------------\n","# 3. Dataset/Dataloader\n","# ----------------------------------------------------------\n","class NumDataset(Dataset):\n","    def __init__(self,pairs,max_len=12):\n","        self.data=[(encode(src),encode(tgt)) for src,tgt in pairs]\n","        self.max_len=max_len\n","    def __len__(self): return len(self.data)\n","    def __getitem__(self,idx):\n","        src,tgt=self.data[idx]\n","        src=[BOS_IDX]+src+[EOS_IDX]\n","        tgt=[BOS_IDX]+tgt+[EOS_IDX]\n","        return torch.tensor(src[:self.max_len]), torch.tensor(tgt[:self.max_len])\n","\n","def collate(batch):\n","    srcs,tgts=zip(*batch)\n","    max_s,max_t=max(len(s) for s in srcs),max(len(t) for t in tgts)\n","    S=torch.full((len(batch),max_s),PAD_IDX)\n","    T=torch.full((len(batch),max_t),PAD_IDX)\n","    for i,(s,t) in enumerate(zip(srcs,tgts)):\n","        S[i,:len(s)],T[i,:len(t)] = s,t\n","    return S.long(),T.long()\n","\n","split=int(0.8*len(pairs))\n","train_ds, val_ds = NumDataset(pairs[:split]), NumDataset(pairs[split:])\n","train_loader=DataLoader(train_ds,batch_size=16,shuffle=True,collate_fn=collate)\n","val_loader=DataLoader(val_ds,batch_size=32,collate_fn=collate)\n","\n","# ----------------------------------------------------------\n","# 4. Modelo Transformer\n","# ----------------------------------------------------------\n","class PositionalEncoding(nn.Module):\n","    def __init__(self,d_model,max_len=5000):\n","        super().__init__()\n","        pe=torch.zeros(max_len,d_model)\n","        pos=torch.arange(0,max_len).unsqueeze(1).float()\n","        div=torch.exp(torch.arange(0,d_model,2).float()*(-math.log(10000)/d_model))\n","        pe[:,0::2]=torch.sin(pos*div); pe[:,1::2]=torch.cos(pos*div)\n","        self.register_buffer(\"pe\",pe)\n","    def forward(self,x): return x+self.pe[:x.size(1)].unsqueeze(0)\n","\n","class MiniTransformer(nn.Module):\n","    def __init__(self,vocab_size,d_model=128,nhead=4,num_layers=2):\n","        super().__init__()\n","        self.d_model=d_model\n","        self.src_emb=nn.Embedding(vocab_size,d_model,padding_idx=PAD_IDX)\n","        self.tgt_emb=nn.Embedding(vocab_size,d_model,padding_idx=PAD_IDX)\n","        self.pos=PositionalEncoding(d_model)\n","        self.transf=nn.Transformer(d_model,nhead,num_layers,num_layers,\n","                                   dim_feedforward=256,batch_first=True)\n","        self.fc=nn.Linear(d_model,vocab_size)\n","    def forward(self,src,tgt):\n","        src_mask=None\n","        tgt_mask=torch.triu(torch.full((tgt.size(1),tgt.size(1)),-float('inf')),1).to(device)\n","        src_key=(src==PAD_IDX); tgt_key=(tgt==PAD_IDX)\n","        src=self.pos(self.src_emb(src)*math.sqrt(self.d_model))\n","        tgt=self.pos(self.tgt_emb(tgt)*math.sqrt(self.d_model))\n","        out=self.transf(src,tgt,tgt_mask=tgt_mask,\n","                        src_key_padding_mask=src_key,\n","                        tgt_key_padding_mask=tgt_key,\n","                        memory_key_padding_mask=src_key)\n","        return self.fc(out)\n","\n","model=MiniTransformer(vocab_size).to(device)\n","optimizer=torch.optim.Adam(model.parameters(),lr=1e-3)\n","criterion=nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","# ----------------------------------------------------------\n","# 5. Entrenamiento\n","# ----------------------------------------------------------\n","def shift_right(t): return t[:,:-1]\n","n_epochs=100\n","for ep in range(1,n_epochs+1):\n","    model.train(); tot=0\n","    for S,T in train_loader:\n","        S,T=S.to(device),T.to(device)\n","        inp=shift_right(T); out=T[:,1:]\n","        logits=model(S,inp)\n","        loss=criterion(logits[:,:out.size(1)].reshape(-1,vocab_size),\n","                       out.reshape(-1))\n","        optimizer.zero_grad(); loss.backward(); optimizer.step()\n","        tot+=loss.item()\n","    if ep%10==0:\n","        print(f\"Epoch {ep} | Train loss {tot/len(train_loader):.3f}\")\n","\n","# ----------------------------------------------------------\n","# 6. Decodificación greedy\n","# ----------------------------------------------------------\n","def greedy_decode(model,src,max_len=15):\n","    model.eval()\n","    src=src.unsqueeze(0).to(device)\n","    tgt=[BOS_IDX]\n","    for _ in range(max_len):\n","        tgt_t=torch.tensor([tgt],device=device)\n","        logits=model(src,tgt_t)\n","        next=logits[0,-1].argmax().item()\n","        if next==EOS_IDX: break\n","        tgt.append(next)\n","    return decode(tgt)\n","\n","# ----------------------------------------------------------\n","# 7. Ejemplos\n","# ----------------------------------------------------------\n","examples=[\"cero\",\"quince\",\"veintidos\",\"treinta y cinco\",\"noventa y nueve\"]\n","for ex in examples:\n","    src=torch.tensor([BOS_IDX]+encode(ex)+[EOS_IDX])\n","    print(f\"{ex:15s} -> {greedy_decode(model,src)}\")\n"]}]}