{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQvl0WKS+I7edGf8Qw9knU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmLMdsd4rnsQ","executionInfo":{"status":"ok","timestamp":1761250058299,"user_tz":-120,"elapsed":36,"user":{"displayName":"Hernan Diaz Rodriguez","userId":"01995553725561234934"}},"outputId":"6118548e-c9dc-4408-f22a-092b954abcc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Matriz de política:\n","v v \n","> ^ \n","--------------------\n","Matriz de valores:\n"," 89.00 100.00 \n","100.00 100.00 \n","--------------------\n"]}],"source":["import numpy as np\n","\n","class GridWorld:\n","    def __init__(self, size=2):\n","        self.size = size\n","        self.n_states = size * size\n","        self.n_actions = 4  # arriba, abajo, izquierda, derecha\n","\n","        # Definir estados especiales\n","        self.start_state = (0, 0)\n","        self.goal_state = (1, 1)\n","        self.obstacles = [(0, 1)]\n","\n","        # Mapeo de acciones a movimientos\n","        self.actions = {\n","            0: (-1, 0),  # arriba\n","            1: (1, 0),   # abajo\n","            2: (0, -1),  # izquierda\n","            3: (0, 1)    # derecha\n","        }\n","\n","        # Crear matrices de transición y recompensa\n","        self.P = self._build_transition_matrix()\n","        self.R = self._build_reward_matrix()\n","\n","    def _pos_to_state(self, pos):\n","        return pos[0] * self.size + pos[1]\n","\n","    def _state_to_pos(self, state):\n","        return (state // self.size, state % self.size)\n","\n","    def _is_valid_position(self, pos):\n","        return 0 <= pos[0] < self.size and 0 <= pos[1] < self.size\n","\n","    def _build_transition_matrix(self):\n","        P = np.zeros((self.n_states, self.n_actions, self.n_states))\n","        for state in range(self.n_states):\n","            pos = self._state_to_pos(state)\n","            if pos == self.goal_state:\n","                for action in range(self.n_actions):\n","                    P[state][action][state] = 1.0\n","                continue\n","            for action in range(self.n_actions):\n","                move = self.actions[action]\n","                new_pos = (pos[0] + move[0], pos[1] + move[1])\n","                if self._is_valid_position(new_pos):\n","                    new_state = self._pos_to_state(new_pos)\n","                    P[state][action][new_state] = 1.0\n","                else:\n","                    P[state][action][state] = 1.0\n","        return P\n","\n","    def _build_reward_matrix(self):\n","        R = np.full((self.n_states, self.n_actions, self.n_states), -1.0)\n","        for state in range(self.n_states):\n","            for action in range(self.n_actions):\n","                for next_state in range(self.n_states):\n","                    if self.P[state][action][next_state] > 0:\n","                        next_pos = self._state_to_pos(next_state)\n","                        if next_pos == self.goal_state:\n","                            R[state][action][next_state] = 10.0\n","                        elif next_pos in self.obstacles:\n","                            R[state][action][next_state] = -5.0\n","                        else:\n","                            R[state][action][next_state] = -1.0\n","        return R\n","\n","    def value_iteration(self, gamma=0.9, theta=1e-6, max_iterations=1000):\n","        V = np.zeros(self.n_states)\n","        for iteration in range(max_iterations):\n","            V_old = V.copy()\n","            for s in range(self.n_states):\n","                action_values = []\n","                for a in range(self.n_actions):\n","                    value = 0\n","                    for s_prime in range(self.n_states):\n","                        prob = self.P[s][a][s_prime]\n","                        reward = self.R[s][a][s_prime]\n","                        value += prob * (reward + gamma * V_old[s_prime])\n","                    action_values.append(value)\n","                V[s] = max(action_values)\n","            if np.max(np.abs(V - V_old)) < theta:\n","                break\n","        return V\n","\n","    def extract_policy(self, V, gamma=0.9):\n","        policy = np.zeros(self.n_states, dtype=int)\n","        for s in range(self.n_states):\n","            action_values = []\n","            for a in range(self.n_actions):\n","                value = 0\n","                for s_prime in range(self.n_states):\n","                    prob = self.P[s][a][s_prime]\n","                    reward = self.R[s][a][s_prime]\n","                    value += prob * (reward + gamma * V[s_prime])\n","                action_values.append(value)\n","            policy[s] = np.argmax(action_values)\n","        return policy\n","\n","    def print_policy_matrix(self, policy):\n","        arrow_symbols = ['^', 'v', '<', '>']\n","        for i in range(self.size):\n","            row_str = \"\"\n","            for j in range(self.size):\n","                state = self._pos_to_state((i, j))\n","                row_str += arrow_symbols[policy[state]] + \" \"\n","            print(row_str)\n","        print(\"-\" * 20)\n","\n","    def print_value_matrix(self, V):\n","        V_grid = V.reshape(self.size, self.size)\n","        for i in range(self.size):\n","            row_str = \"\"\n","            for j in range(self.size):\n","                row_str += f\"{V_grid[i,j]:6.2f} \"\n","            print(row_str)\n","        print(\"-\" * 20)\n","\n","\n","# === Uso ===\n","grid = GridWorld(size=2)  # cambia tamaño si quieres\n","\n","V_optimal = grid.value_iteration(gamma=0.9)\n","policy_optimal = grid.extract_policy(V_optimal, gamma=0.9)\n","\n","print(\"Matriz de política:\")\n","grid.print_policy_matrix(policy_optimal)\n","\n","print(\"Matriz de valores:\")\n","grid.print_value_matrix(V_optimal)"]}]}