{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMx4GMS335gvbns+NpXbt3I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybJmzbKz7V6X","executionInfo":{"status":"ok","timestamp":1761221118782,"user_tz":-120,"elapsed":47,"user":{"displayName":"Hernan Diaz Rodriguez","userId":"01995553725561234934"}},"outputId":"41b385dd-f603-41b6-b37f-f8498b9ffe1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Valores Q finales estimados: [0.1875     0.8        0.77777778 0.25       0.90491118]\n","Valores verdaderos: [0.2, 0.8, 0.5, 0.3, 0.9]\n","Recompensa promedio: 0.886\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Clase que representa el entorno del problema del bandido (multi-armed bandit)\n","class BanditEnvironment:\n","   def __init__(self):\n","       # Inicialización de las probabilidades de recompensa reales para cada brazo\n","       self.true_rewards = [0.2, 0.8, 0.5, 0.3, 0.9]\n","\n","   # Método que simula la ejecución de una acción (tirar de un brazo)\n","   def step(self, action):\n","       \"\"\"Ejecuta acción y devuelve recompensa\"\"\"\n","       # Genera una recompensa binaria (1 o 0) basada en la probabilidad del brazo seleccionado\n","       reward = 1 if np.random.random() < self.true_rewards[action] else 0\n","       return reward\n","\n","# Clase que implementa un agente con estrategia epsilon-greedy\n","class EpsilonGreedyAgent:\n","   def __init__(self, n_actions, epsilon=0.1, decay_rate=0.99):\n","       # Número de acciones posibles (brazos del bandido)\n","       self.n_actions = n_actions\n","       # Probabilidad inicial de exploración\n","       self.epsilon = epsilon\n","       # Tasa de decaimiento para epsilon\n","       self.decay_rate = decay_rate\n","       # Valores Q estimados para cada acción (inicializados a 0)\n","       self.q_values = np.zeros(n_actions)\n","       # Contador de cuántas veces se ha seleccionado cada acción\n","       self.action_counts = np.zeros(n_actions)\n","\n","   # Método para seleccionar una acción según la estrategia epsilon-greedy\n","   def select_action(self):\n","       # Con probabilidad epsilon, selecciona una acción aleatoria (exploración)\n","       if np.random.random() < self.epsilon:\n","           return np.random.randint(self.n_actions)\n","       # Con probabilidad 1-epsilon, selecciona la mejor acción conocida (explotación)\n","       else:\n","           return np.argmax(self.q_values)\n","\n","   # Método para actualizar la estimación del valor Q de una acción\n","   def update_q_value(self, action, reward):\n","       # Incrementa el contador de la acción seleccionada\n","       self.action_counts[action] += 1\n","       # Actualiza el valor Q usando un promedio incremental\n","       self.q_values[action] += (reward - self.q_values[action]) / self.action_counts[action]\n","\n","   # Método para reducir gradualmente epsilon (menos exploración con el tiempo)\n","   def decay_epsilon(self):\n","       self.epsilon *= self.decay_rate\n","\n","# Función que simula el problema del bandido con el agente epsilon-greedy\n","def simulate_bandit_problem():\n","   # Creación del entorno con las recompensas reales\n","   env = BanditEnvironment()\n","   # Creación del agente con 5 acciones y epsilon inicial de 0.3\n","   agent = EpsilonGreedyAgent(n_actions=5, epsilon=0.3)\n","\n","   # Listas para almacenar el historial de recompensas y acciones\n","   rewards_history = []\n","   actions_history = []\n","\n","   # Bucle principal de simulación (1000 pasos)\n","   for step in range(1000):\n","       # 1. El agente selecciona una acción\n","       action = agent.select_action()\n","\n","       # 2. El entorno devuelve una recompensa para la acción\n","       reward = env.step(action)\n","\n","       # 3. El agente actualiza sus estimaciones con la recompensa recibida\n","       agent.update_q_value(action, reward)\n","       # Reducción gradual de epsilon\n","       agent.decay_epsilon()\n","\n","       # Almacenamiento del historial para análisis posterior\n","       rewards_history.append(reward)\n","       actions_history.append(action)\n","\n","   return rewards_history, actions_history, agent.q_values\n","\n","# Ejecución de la simulación y obtención de resultados\n","rewards, actions, final_q_values = simulate_bandit_problem()\n","\n","# Impresión de resultados finales\n","print(\"Valores Q finales estimados:\", final_q_values)\n","print(\"Valores verdaderos:\", [0.2, 0.8, 0.5, 0.3, 0.9])\n","print(\"Recompensa promedio:\", np.mean(rewards))"]}]}
